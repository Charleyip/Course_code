{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zNmLmqrJAXXp"
   },
   "source": [
    "# LIS 640 Applied Deep Learning : Backpropagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O3EvIZ0uAOVN"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hbe3wUpVAjma"
   },
   "source": [
    "# Implementing a Two-Layer Network\n",
    "In this exercise we will develop a Two-Layer Network with fully-connected layers to perform classification, and test it out on the MNIST dataset.\n",
    "\n",
    "We train the network with a Cross-Entropy loss function. The network uses a ReLU activation function after the first fully connected layer. \n",
    "\n",
    "In other words, the network has the following architecture:\n",
    "\n",
    "  input - fully connected layer - ReLU - fully connected layer - Softmax - Cross-Entropy\n",
    "\n",
    "We denote the input example as $x$, the ground truth label as $y$, the first fully connected layer output as $h$, the activation function output as $a$ and the second fully connected layer output as $s$. The complete process of the two-layer network is illustrated below.\n",
    "\n",
    "$h = W_1^Tx$\n",
    "\n",
    "$a = ReLU(h)$\n",
    "\n",
    "$s = W_2^Ta$\n",
    "\n",
    "$loss = CrossEntropy(softmax(s),y)$ \n",
    "\n",
    "Here $softmax(z)_i=\\frac{e^{z_i}}{\\sum^K_{j=1}e^{z_j}}$ and $CrossEntropy(y',y)=-\\sum_i y'_i log(y_i)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lJqim3P1qZgv"
   },
   "source": [
    "## Play with a toy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5T-4Phbd9GvI"
   },
   "source": [
    "The inputs to our network will be a batch of $N$ (`num_inputs`) $D$-dimensional vectors (`input_size`); the hidden layer will have $H$ hidden units (`hidden_size`), and we will predict classification scores for $C$ categories (`num_classes`). This means that the learnable weights of the network will have the following shapes:\n",
    "\n",
    "*   W1: First layer weights; has shape (D, H)\n",
    "*   W2: Second layer weights; has shape (H, C)\n",
    "\n",
    "We will use `utils.get_toy_data` function to generate random weights for a small toy model while we implement the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZLdCF3B-AOVT"
   },
   "source": [
    "### Forward pass: compute scores\n",
    "We want to write a function that takes as input the model weights and a batch of images and labels, and returns the loss and the gradient of the loss with respect to each model parameter.\n",
    "\n",
    "However rather than attempting to implement the entire function at once, we will take a staged approach and ask you to implement the full forward and backward pass one step at a time.\n",
    "\n",
    "First we will implement the forward pass of the network which uses the weights and biases to compute scores for all inputs in `nn_forward_pass`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "inlH2l-XEtZQ"
   },
   "source": [
    "Compute the scores and compare with the answer. The distance gap should be smaller than 1e-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch cannot use GPUs.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  print('PyTorch can use GPUs!')\n",
    "else:\n",
    "  print('PyTorch cannot use GPUs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "tZV9_3ZWAOVU",
    "outputId": "7504b688-c002-4676-c064-29adc38f88a4"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\HW 2\\problem2.ipynb Cell 10\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/HW%202/problem2.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mproblem2\u001b[39;00m \u001b[39mimport\u001b[39;00m nn_forward_pass\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/HW%202/problem2.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m utils\u001b[39m.\u001b[39mreset_seed(\u001b[39m0\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/HW%202/problem2.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m toy_X, toy_y, params \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mget_toy_data()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/HW%202/problem2.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# YOUR_TURN: Implement the score computation part of nn_forward_pass\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/HW%202/problem2.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m scores, _ \u001b[39m=\u001b[39m nn_forward_pass(params, toy_X)\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\HW 2\\utils.py:57\u001b[0m, in \u001b[0;36mget_toy_data\u001b[1;34m(num_inputs, input_size, hidden_size, num_classes, dtype, device)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# Generate some random parameters, storing them in a dict\u001b[39;00m\n\u001b[0;32m     56\u001b[0m params \u001b[39m=\u001b[39m {}\n\u001b[1;32m---> 57\u001b[0m params[\u001b[39m\"\u001b[39m\u001b[39mW1\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1e-4\u001b[39m \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39;49mrandn(D, H, device\u001b[39m=\u001b[39;49mdevice, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m     58\u001b[0m params[\u001b[39m\"\u001b[39m\u001b[39mW2\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1e-4\u001b[39m \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mrandn(H, C, device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m     60\u001b[0m \u001b[39m# Generate some random inputs and labels\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    285\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m     )\n\u001b[0;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m\"\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    290\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    291\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    292\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    293\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from problem2 import nn_forward_pass\n",
    "\n",
    "utils.reset_seed(0)\n",
    "toy_X, toy_y, params = utils.get_toy_data()\n",
    "\n",
    "# YOUR_TURN: Implement the score computation part of nn_forward_pass\n",
    "scores, _ = nn_forward_pass(params, toy_X)\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print(scores.dtype)\n",
    "print()\n",
    "print('correct scores:')\n",
    "correct_scores = torch.tensor([\n",
    "        [ 9.7003e-08, -1.1143e-07, -3.9961e-08],\n",
    "        [-7.4297e-08,  1.1502e-07,  1.5685e-07],\n",
    "        [-2.5860e-07,  2.2765e-07,  3.2453e-07],\n",
    "        [-4.7257e-07,  9.0935e-07,  4.0368e-07],\n",
    "        [-1.8395e-07,  7.9303e-08,  6.0360e-07]], dtype=torch.float32, device=scores.device)\n",
    "print(correct_scores)\n",
    "print()\n",
    "\n",
    "# The difference should be very small. We get < 1e-10\n",
    "scores_diff = (scores - correct_scores).abs().sum().item()\n",
    "print('Difference between your scores and correct scores: %.2e' % scores_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7XNJ3ydEAOVW"
   },
   "source": [
    "### Forward pass: compute loss\n",
    "Now, we implement the first part of `nn_forward_backward` that computes the loss.\n",
    "\n",
    "For the data loss, we compute the Cross-Entropy loss. Note that the final loss shold be an average loss of $N$ input examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C734SdJGE6xh"
   },
   "source": [
    "First, Let's run the following to check your implementation.\n",
    "\n",
    "We compute the loss for the toy data, and compare with the answer computed by our implementation. The difference between the correct and computed loss should be less than `1e-4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "wgG6w2uKAOVX",
    "outputId": "e198ce0f-1f05-431e-e724-240aeaa09b5e"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "from problem2 import nn_forward_backward\n",
    "\n",
    "utils.reset_seed(0)\n",
    "toy_X, toy_y, params = utils.get_toy_data()\n",
    "\n",
    "# YOUR_TURN: Implement the loss computation part of nn_forward_backward\n",
    "loss, _ = nn_forward_backward(params, toy_X, toy_y)\n",
    "print('Your loss: ', loss.item())\n",
    "correct_loss = 1.0986121892929077\n",
    "print('Correct loss: ', correct_loss)\n",
    "diff = (correct_loss - loss).item()\n",
    "\n",
    "# should be very small, we get < 1e-4\n",
    "print('Difference: %.4e' % diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vExP-7n3AOVa"
   },
   "source": [
    "### Backward pass\n",
    "Now implement the backward pass for the entire network in `nn_forward_backward`.\n",
    "\n",
    "After doing so, we will use numeric gradient checking to see whether the analytic gradient computed by our backward pass mateches a numeric gradient.\n",
    "\n",
    "We will use the functions `utils.compute_numeric_gradient` and `utils.rel_error` to help with numeric gradient checking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: For gradient computation of Softmax Cross-Entropy loss, please refer to https://www.michaelpiseno.com/blog/2021/softmax-gradient/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "93oOdibtW_Kl"
   },
   "source": [
    "Now we will compute the gradient of the loss with respect to the variables `W1` and `W2`. Now that you (hopefully!) have a correctly implemented forward pass, you can debug your backward pass using a numeric gradient check.\n",
    "\n",
    "You should see relative errors less than `1e-4` for all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "qCEkprvoAOVb",
    "outputId": "3e02d110-e672-4e33-80b8-5d898cfb30ef"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "from problem2 import nn_forward_backward\n",
    "\n",
    "utils.reset_seed(0)\n",
    "\n",
    "toy_X, toy_y, params = utils.get_toy_data(dtype=torch.float64)\n",
    "\n",
    "# YOUR_TURN: Implement the gradient computation part of nn_forward_backward\n",
    "#            When you implement the gradient computation part, you may need to \n",
    "#            implement the `hidden` output in nn_forward_pass, as well.\n",
    "loss, grads = nn_forward_backward(params, toy_X, toy_y)\n",
    "\n",
    "for param_name, grad in grads.items():\n",
    "  param = params[param_name]\n",
    "  f = lambda w: nn_forward_backward(params, toy_X, toy_y)[0]\n",
    "  grad_numeric = utils.compute_numeric_gradient(f, param)\n",
    "  error = utils.rel_error(grad, grad_numeric)\n",
    "  print('%s max relative error: %e' % (param_name, error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LjAUalCBAOVd"
   },
   "source": [
    "### Train the network\n",
    "To train the network we will use stochastic gradient descent (SGD). \n",
    "\n",
    "Look at the function `nn_train` and fill in the missing sections to implement the training procedure.\n",
    "\n",
    "Once you have implemented the method, run the code below to train a two-layer network on toy data. Your final training loss should be less than 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "Wgw06cLXAOVd",
    "outputId": "be163c99-6590-4354-eafa-93d623bed3a8"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "from utils import get_toy_data\n",
    "from problem2 import nn_forward_backward, nn_train, nn_predict\n",
    "\n",
    "utils.reset_seed(0)\n",
    "toy_X, toy_y, params = get_toy_data()\n",
    "\n",
    "# YOUR_TURN: Implement the nn_train function.\n",
    "#            You may need to check nn_predict function (the \"pred_func\") as well.\n",
    "stats = nn_train(params, nn_forward_backward, nn_predict, toy_X, toy_y, toy_X, toy_y,\n",
    "                 learning_rate=1e-1, reg=1e-6,\n",
    "                 num_iters=200, verbose=False)\n",
    "\n",
    "print('Final training loss: ', stats['loss_history'][-1])\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(stats['loss_history'], 'o')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8cPIajWNAOVg"
   },
   "source": [
    "## Testing our NN on a real dataset: MNIST\n",
    "Now that you have implemented a two-layer network that passes gradient checks and works on toy data, it's time to load up our MNIST data so we can use it to train a classifier on a real dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "colab_type": "code",
    "id": "lYo_XrU3AOVg",
    "outputId": "e0e8ca93-3570-45f4-96ec-d03d91b1148c"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "utils.reset_seed(0)\n",
    "x_train, y_train, x_val, y_val = utils.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cq-HkgRBAOVQ"
   },
   "source": [
    "### Wrap all function into a Class\n",
    "We will use the class `TwoLayerNet` to represent instances of our network. The network parameters are stored in the instance variable `self.params` where keys are string parameter names and values are PyTorch tensors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_CsYAv3uAOVi"
   },
   "source": [
    "### Train a network\n",
    "To train our network we will use SGD. In addition, we will adjust the learning rate with an exponential learning rate schedule as optimization proceeds; after each epoch, we will reduce the learning rate by multiplying it by a decay rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "hgg0QV9DAOVj",
    "outputId": "ac949f3a-edf9-4a54-c47a-4c348c6c89e2"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "from problem2 import TwoLayerNet\n",
    "\n",
    "input_size = 1 * 28 * 28\n",
    "hidden_size = 36\n",
    "num_classes = 10\n",
    "\n",
    "# fix random seed before we generate a set of parameters\n",
    "utils.reset_seed(0)\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes, dtype=torch.float32, device='cpu')\n",
    "\n",
    "# Train the network\n",
    "stats = net.train(x_train, y_train,\n",
    "                  x_val, y_val,\n",
    "                  num_iters=500, batch_size=1000,\n",
    "                  learning_rate=1e-1, learning_rate_decay=0.95,\n",
    "                  verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = net.predict(x_val)\n",
    "val_acc = 100.0 * (y_val_pred == y_val).double().mean().item()\n",
    "print('Validation accuracy: %.2f%%' % val_acc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "eYE9thuXn4zP",
    "CdowvtJen-IP",
    "KtMy3qeipNK3",
    "Hbe3wUpVAjma",
    "lJqim3P1qZgv",
    "ZLdCF3B-AOVT",
    "7XNJ3ydEAOVW",
    "vExP-7n3AOVa",
    "LjAUalCBAOVd",
    "8cPIajWNAOVg",
    "_CsYAv3uAOVi",
    "ixxgq5RKAOVl",
    "OlVbXxmPNzPY",
    "rDNZ8ZAnN7hj",
    "QpSrK3olUfOZ",
    "3zFWkxebWXtu",
    "mVCEro4FAOVq",
    "UG56gKWsAOVv",
    "37R_J2uMP3d-"
   ],
   "name": "two_layer_net.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
