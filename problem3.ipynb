{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "DDJwQPZcupab",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# LIS 640 Applied Deep Learning : Computation Graphs in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "3Qiu9_4pe1CP",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "In this exercise we will implement two-layer network using a more modular approach. For each layer we will implement a `forward` and a `backward` function. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive dout (derivative of loss with respect to outputs) and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```\n",
    "\n",
    "After implementing a bunch of layers this way, we will be able to easily combine them to build classifiers with different architectures. Your task here is to implement `ReLU` activation function with modular approach.\n",
    "\n",
    "\n",
    "To validate our implementation, we will compare the analytically computed gradients with numerical approximations of the gradient as done in previous assignments. You can inspect the numeric gradient function `utils.compute_numeric_gradient`. Please note that we have updated the function to accept upstream gradients to allow us to debug intermediate layers easily.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "bdIqQzqiJQE6",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# ReLU activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "YdX98A_qvTRt",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We will now implement the ReLU activation function. As above, we will define a class with two empty static methods, and implement them in upcoming cells. The class structure can be found in `problem3.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "n2DyqL4Ae1Cl",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## ReLU activation: forward\n",
    "Implement the forward pass for the ReLU activation function in the `ReLU.forward` function. You **should not** change the input tensor with an in-place operation. \n",
    "\n",
    "Run the following to test your implementation of the ReLU forward pass. Your errors should be less than `1e-7`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "deletable": true,
    "executionInfo": {
     "elapsed": 5207,
     "status": "ok",
     "timestamp": 1601234378129,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 240
    },
    "id": "QblpieUJe1Cm",
    "new_sheet": false,
    "outputId": "3e7cf951-3c8f-44eb-b5da-ba72042d5184",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ReLU.forward function:\n",
      "difference:  4.5454545613554664e-09\n"
     ]
    }
   ],
   "source": [
    "from problem3 import ReLU\n",
    "\n",
    "utils.reset_seed(0)\n",
    "x = torch.linspace(-0.5, 0.5, steps=12, dtype=torch.float64, device='cuda')\n",
    "x = x.reshape(3, 4)\n",
    "\n",
    "out, _ = ReLU.forward(x)\n",
    "correct_out = torch.tensor([[ 0.,          0.,          0.,          0.,        ],\n",
    "                            [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                            [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]],\n",
    "                            dtype=torch.float64,\n",
    "                            device='cuda')\n",
    "\n",
    "# Compare your output with ours. The error should be on the order of e-8\n",
    "print('Testing ReLU.forward function:')\n",
    "print('difference: ', utils.rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "3bSInb7xe1Cq",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## ReLU activation: backward\n",
    "Now implement the backward pass for the ReLU activation function.\n",
    "\n",
    "Again, you should not change the input tensor with an in-place operation.\n",
    "\n",
    "Run the following to test your implementation of `ReLU.backward`. Your errors should be less than `1e-8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "deletable": true,
    "executionInfo": {
     "elapsed": 5190,
     "status": "ok",
     "timestamp": 1601234378129,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 240
    },
    "id": "odiV48zBe1Cr",
    "new_sheet": false,
    "outputId": "710d6e24-1422-4684-92f2-b40f3f68cbd7",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ReLU.backward function:\n",
      "dx error:  2.6317796097761553e-10\n"
     ]
    }
   ],
   "source": [
    "from problem3 import ReLU\n",
    "\n",
    "utils.reset_seed(0)\n",
    "x = torch.randn(10, 10, dtype=torch.float64, device='cuda')\n",
    "dout = torch.randn(*x.shape, dtype=torch.float64, device='cuda')\n",
    "\n",
    "dx_num = utils.compute_numeric_gradient(lambda x: ReLU.forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = ReLU.forward(x)\n",
    "dx = ReLU.backward(dout, cache)\n",
    "\n",
    "# The error should be on the order of e-12\n",
    "print('Testing ReLU.backward function:')\n",
    "print('dx error: ', utils.rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "id": "qq7-cyfQe1C4",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Two-layer network\n",
    "In the previous problem2 you implemented a two-layer neural network in a single monolithic class. Now that you have implemented modular versions of the necessary layers, you will reimplement the two layer network using these modular implementations.\n",
    "\n",
    "Complete the implementation of the `TwoLayerNet` class. This class will serve as a model for the other networks you will implement in this assignment, so read through it to make sure you understand the API. \n",
    "\n",
    "Once you have finished implementing the forward and backward passes of your two-layer net, run the following to test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "deletable": true,
    "executionInfo": {
     "elapsed": 7734,
     "status": "ok",
     "timestamp": 1601234380728,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 240
    },
    "id": "d3JOcfyze1C5",
    "new_sheet": false,
    "outputId": "2d38bb9e-f8fe-4a58-dc79-f817bab4949e",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing initialization ... \n",
      "Testing test-time forward pass ... \n",
      "Testing training loss (no regularization)\n",
      "Running numeric gradient check:\n",
      "W1 relative error: 1.87e-07\n",
      "W2 relative error: 1.46e-09\n"
     ]
    }
   ],
   "source": [
    "from problem3 import TwoLayerNet\n",
    "torch.set_printoptions(precision=12, threshold=None, edgeitems=None, linewidth=None, profile=None)\n",
    "\n",
    "utils.reset_seed(0)\n",
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = torch.randn(N, D, dtype=torch.float64, device='cuda')\n",
    "y = torch.randint(C, size=(N,), dtype=torch.int64, device='cuda')\n",
    "\n",
    "std = 1e-3\n",
    "model = TwoLayerNet(\n",
    "          input_dim=D,\n",
    "          hidden_dim=H,\n",
    "          num_classes=C,\n",
    "          weight_scale=std,\n",
    "          dtype=torch.float64,\n",
    "          device='cuda'\n",
    "        )\n",
    "\n",
    "print('Testing initialization ... ')\n",
    "W1_std = torch.abs(model.params['W1'].std() - std)\n",
    "W2_std = torch.abs(model.params['W2'].std() - std)\n",
    "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
    "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
    "\n",
    "print('Testing test-time forward pass ... ')\n",
    "model.params['W1'] = torch.linspace(-0.7, 0.3, steps=D * H, dtype=torch.float64, device='cuda').reshape(D, H)\n",
    "model.params['W2'] = torch.linspace(-0.3, 0.4, steps=H * C, dtype=torch.float64, device='cuda').reshape(H, C)\n",
    "X = torch.linspace(-5.5, 4.5, steps=N * D, dtype=torch.float64, device='cuda').reshape(D, N).t()\n",
    "scores = model.loss(X)\n",
    "correct_scores = torch.tensor(\n",
    "        [[ 8.56847057,  9.12177260,  9.67507463, 10.22837667, 10.78167870,\n",
    "         11.33498073, 11.88828277],\n",
    "        [ 9.09451046,  9.57617926, 10.05784805, 10.53951685, 11.02118564,\n",
    "         11.50285444, 11.98452323],\n",
    "        [ 9.62055036, 10.03058591, 10.44062147, 10.85065703, 11.26069259,\n",
    "         11.67072814, 12.08076370]],\n",
    "    dtype=torch.float64, device='cuda')\n",
    "scores_diff = torch.abs(scores - correct_scores).sum()\n",
    "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
    "\n",
    "print('Testing training loss (no regularization)')\n",
    "y = torch.tensor([0, 5, 1])\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 2.881451052641\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
    "\n",
    "# Errors should be around e-6 or less\n",
    "print('Running numeric gradient check:')\n",
    "loss, grads = model.loss(X, y)\n",
    "\n",
    "for name in sorted(grads):\n",
    "  f = lambda _: model.loss(X, y)[0]\n",
    "  grad_num = utils.compute_numeric_gradient(f, model.params[name])\n",
    "  print('%s relative error: %.2e' % (name, utils.rel_error(grad_num, grads[name])))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "fully_connected_networks.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
